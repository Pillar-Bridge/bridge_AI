{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/bmegpu02/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "api_key = os.getenv('api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 ms, sys: 4.29 ms, total: 6.45 ms\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.generate_content(\"카페에서 커피를 마시고 싶을 때 사용하는 멘트는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \"아메리카노 한 잔 주세요.\"\n",
      "* \"라떼 한 잔 주세요. 탈지유로 해주세요.\"\n",
      "* \"플랫 화이트 한 잔 주세요.\"\n",
      "* \"카페 모카 한 잔 주세요. 무가당으로 해주세요.\"\n",
      "* \"카라멜 마키아토 한 잔 주세요. 아이스로 해주세요.\"\n",
      "* \"에스프레소 샷 두 잔 주세요.\"\n",
      "* \"차가운 브루 한 잔 주세요.\"\n",
      "* \"아이ced Americano 한 잔 주세요. 스위트너 있게 해주세요.\"\n",
      "* \"니트로 콜드 브루 한 잔 주세요.\"\n",
      "* \"필터 커피 한 잔 주세요. 블랙으로 해주세요.\"\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'place': '카페',\n",
       "  'question': '안녕하세요. 주문하시겠어요?',\n",
       "  'add_tag': '주문하는 상황',\n",
       "  'anohter_tag': '추가 질문 없음',\n",
       "  'lang': 'ko'},\n",
       " {'role': 'user',\n",
       "  'place': '카페',\n",
       "  'question': '안녕하세요. 주문하시겠어요?',\n",
       "  'add_tag': '주문하는 상황',\n",
       "  'anohter_tag': '추가 질문 없음',\n",
       "  'lang': 'ko'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import json \n",
    "api_key = os.getenv('api_key')\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "# 특정 파일에 저장된 사용자 질문을 읽어들여서, 모델에 전달하고, 모델의 응답을 출력하는 코드\n",
    "# 파일의 형식은 다음과 같다.\n",
    "# [\n",
    "#     {\n",
    "#         \"role\":\"user\",\n",
    "#         \"parts\": [\"특정 장소에서 특정 질문에 대한 화자의 답변을 출력하도록 해주세요.\"]\n",
    "#     },\n",
    "#     {\n",
    "#         \"role\":\"user\",\n",
    "#         \"parts\": [\"특정장소(카페)에서 특정 질문(손님 주문하시겠어요?라는 질문에 대한 답변을 해주세요.\"]\n",
    "#     }\n",
    "# ]\n",
    "# 파일을 읽어들여서, 사용자 질문을 모델에 전달하고, 모델의 응답을 출력한다.\n",
    "file_path = os.path.join('./data/230217.json')\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    user_queries = json.load(f)\n",
    "\n",
    "user_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user_queries = [\n",
    "    {\n",
    "        'role':'user',\n",
    "        'parts': [f\"특정장소(카페)에서 특정 질문(손님 주문하시겠어요?)라는 질문에 대한 답변하기 위한 멘트를 나열 해주세요.\"]\n",
    "    },\n",
    "    {\n",
    "        'role':'user',\n",
    "        'parts': [\"추가로 카페에서 잘나가는 메뉴를 추천받기 위해 사용하는 멘트를 나열해주세요\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "history = []\n",
    "for user_query in user_queries:\n",
    "    history.append(user_query)\n",
    "    print(f'[사용자]: {user_query[\"parts\"][0]}')   \n",
    "    response = model.generate_content(history)\n",
    "    print(f'[모델]: {response.text}')   \n",
    "    history.append(response.candidates[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특정장소(카페)에서 특정 질문(손님 주문하시겠어요?)라는 질문에 대한 답변하기 위한 멘트를 나열 해주세요.\n",
      "추가로 상대방이 특정 질문(안에서 드시고 가시나요?)을 했을 때, 답변하기 위한 멘트를 나열해주세요.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import json \n",
    "genai.configure(api_key='AIzaSyCKv73cpWOu7mWuvgoMVm3xp__9b4TpzIE')\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "#########################################################\n",
    "# 파일을 읽어들여서, 사용자 질문을 모델에 전달하고, 모델의 응답을 출력한다.\n",
    "file_path = os.path.join('./data/230217.json')\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    user_queries = json.load(f)\n",
    "\n",
    "user_queries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[사용자]: 특정장소(카페)에서 특정 질문(손님 주문하시겠어요?)라는 질문에 대한 답변하기 위한 멘트를 나열 해주세요.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Could not recognize the intended type of the `dict`. A `Content` should have a 'parts' key. A `Part` should have a 'inline_data' or a 'text' key. A `Blob` should have 'mime_type' and 'data' keys. Got keys: ['role', 'place', 'question', 'add_tag', 'anohter_tag', 'lang']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1096865/3258300451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[사용자]: {user_query[\"question\"][0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[모델]: {response.text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     ) -> generation_types.GenerateContentResponse:\n\u001b[0;32m--> 234\u001b[0;31m         request = self._prepare_request(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36m_prepare_request\u001b[0;34m(self, contents, generation_config, safety_settings, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"contents must not be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_generation_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/types/content_types.py\u001b[0m in \u001b[0;36mto_contents\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# strict_to_content so [[parts], [parts]] doesn't assume roles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstrict_to_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/types/content_types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# strict_to_content so [[parts], [parts]] doesn't assume roles.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstrict_to_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/types/content_types.py\u001b[0m in \u001b[0;36mstrict_to_content\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstrict_to_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStrictContentType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/google/generativeai/types/content_types.py\u001b[0m in \u001b[0;36m_convert_dict\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         raise KeyError(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;34m\"Could not recognize the intended type of the `dict`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;34m\"A `Content` should have a 'parts' key. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Could not recognize the intended type of the `dict`. A `Content` should have a 'parts' key. A `Part` should have a 'inline_data' or a 'text' key. A `Blob` should have 'mime_type' and 'data' keys. Got keys: ['role', 'place', 'question', 'add_tag', 'anohter_tag', 'lang']\""
     ]
    }
   ],
   "source": [
    "\n",
    "########################################################\n",
    "\n",
    "    \n",
    "history = []\n",
    "for user_query in user_queries:\n",
    "    print(f'[사용자]: {user_query[\"question\"][0]}')\n",
    "\n",
    "    response = model.generate_content(history)    \n",
    "    print(f'[모델]: {response.text}')\n",
    "    history.append(response.candidates[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 매개변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 소요 시간 : 12.845218896865845초\n",
      "canidate 생성 건수: 1\n",
      "Token 수 : total_tokens: 109\n",
      "\n",
      "[User] : 특정장소(카페)에서 상대방이 특정 질문(안녕하세요. 주문하시겠어요?)라는 질문에 대해 어떤 상황인지 1줄로 설명하고, 그에 대한 답변을 5개 생성해주세요. 더불어 답변에 대한 유의어 문장과 반의어 문장을 각각 생성해주세요. 이때 한국어로 만들어주시고, json 파일로 저장해주세요.\n",
      "[AI] : ```json\n",
      "{\n",
      "  \"상황\": \"카페에서 주문을 받는 상황\",\n",
      "  \"질문\": \"안녕하세요. 주문하시겠어요?\",\n",
      "  \"답변\": [\n",
      "    {\n",
      "      \"답변\": \"네, 주문하겠습니다.\",\n",
      "      \"유의어 문장\": \"주문을 부탁드릴게요.\",\n",
      "      \"반의어 문장\": \"아니요, 주문하지 않겠습니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"답변\": \"아직 주문을 고민 중이에요.\",\n",
      "      \"유의어 문장\": \"메뉴를 좀 더 살펴보고 싶습니다.\",\n",
      "      \"반의어 문장\": \"주문을 이미 결정했습니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"답변\": \"잠깐만요. 친구가 올 때까지 기다려 주세요.\",\n",
      "      \"유의어 문장\": \"친구와 함께 주문하고 싶습니다.\",\n",
      "      \"반의어 문장\": \"혼자 주문할게요.\"\n",
      "    },\n",
      "    {\n",
      "      \"답변\": \"메뉴에 없는 것을 주문하고 싶은데 가능한지요?\",\n",
      "      \"유의어 문장\": \"맞춤형 주문을 부탁드릴 수 있나요?\",\n",
      "      \"반의어 문장\": \"메뉴에 있는 것만 주문할게요.\"\n",
      "    },\n",
      "    {\n",
      "      \"답변\": \"주문하기 전에 몇 가지 질문이 있어요.\",\n",
      "      \"유의어 문장\": \"메뉴에 대해 자세히 알고 싶습니다.\",\n",
      "      \"반의어 문장\": \"주문할 준비가 되었습니다.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import google.generativeai as genai \n",
    "import json\n",
    "start_time = time.time()\n",
    "genai.configure(api_key='AIzaSyCKv73cpWOu7mWuvgoMVm3xp__9b4TpzIE')\n",
    "generation_config = genai.GenerationConfig(\n",
    "    candidate_count=1,\n",
    "    top_k = 5,\n",
    "    top_p = 0.1, # 0~1 사이의 값으로, 0에 가까울수록 고정적 답변, 1에 가까울수록 창의적인 답변\n",
    "    temperature= 0 # 0~1 사이의 값으로, 0에 가까울수록 고정적 답변, 1에 가까울수록 창의적인 답변\n",
    "    # stop_sequences = [\". \", \"! \"] #stop_sequnece에 있는 문자열을 만나면 생성을 중단시킴 \n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro', generation_config=generation_config)\n",
    "####################################################################################\n",
    "\n",
    "def inverted_lang(language):\n",
    "    if language == 'ko':\n",
    "        return '한국어'\n",
    "    else:\n",
    "        return '영어'\n",
    "    \n",
    "json_file = json.load(open('../input/testInput.json', 'r', encoding='utf-8'))\n",
    "\n",
    "place = json_file['place']\n",
    "question = json_file['text']\n",
    "lang = inverted_lang(json_file['lang'])\n",
    "\n",
    "\n",
    "query = f\"특정장소({place})에서 상대방이 특정 질문({question})라는 질문에 대해 어떤 상황인지 1줄로 설명하고, 그에 대한 답변을 5개 생성해주세요. 더불어 답변에 대한 비슷한 종류 문장과 반의어 문장을 각각 생성해주세요. 이때 한국어로 만들어주시고, json 파일로 저장해주세요.\"\n",
    "\n",
    "tokens = model.count_tokens(query) # Token 수 측정\n",
    "\n",
    "response = model.generate_content(query)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time # 경과시간 계산 \n",
    "print(f\"총 소요 시간 : {elapsed_time}초\")\n",
    "print(f\"canidate 생성 건수: {len(response.candidates)}\")\n",
    "print(f\"Token 수 : {tokens}\") #Eng : 1토큰 = 4글자, Kor : 1토큰 = 1.5글자\n",
    "print(f\"[User] : {query}\")\n",
    "print(f\"[AI] : {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[사용자]: 특정장소(카페)에서 점원이 특정 질문(손님 주문하시겠어요?)라는 질문에 대해 어떤 상황인지 1줄로 설명\n",
      "[모델]: 카페에서 점원이 손님에게 주문을 묻는 상황\n",
      "[사용자]: 이 상황에서 손님이 주문하기 위한 멘트를 나열 해주세요. 이때 멘트는 5개 안으로 제한해주시고, 총 2가지 상황(주문, 용무)에 대한 멘트를 나열해주세요. 마지막으로 json 파일 형식으로 만들어주세요\n",
      "[모델]: ```json\n",
      "{\n",
      "  \"주문\": [\n",
      "    \"아메리카노 주세요.\",\n",
      "    \"카페라떼 한 잔 주문하고 싶습니다.\",\n",
      "    \"티라미수와 함께 카푸치노 주세요.\",\n",
      "    \"아이스티 한 잔 주세요.\",\n",
      "    \"케이크 세트 하나 주문할게요.\"\n",
      "  ],\n",
      "  \"용무\": [\n",
      "    \"화장실 빌려주실 수 있나요?\",\n",
      "    \"와이파이 비밀번호 알려주실 수 있나요?\",\n",
      "    \"콘센트 빌려주실 수 있나요?\",\n",
      "    \"책 좀 빌려볼 수 있나요?\",\n",
      "    \"이곳에서 공부해도 될까요?\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai \n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "    candidate_count=1,\n",
    "    top_k = 5,\n",
    "    top_p = 0, # 0~1 사이의 값으로, 0에 가까울수록 고정적 답변, 1에 가까울수록 창의적인 답변\n",
    "    temperature= 0.5 # 0~1 사이의 값으로, 0에 가까울수록 고정적 답변, 1에 가까울수록 창의적인 답변\n",
    "    # stop_sequences = [\". \", \"! \"] #stop_sequnece에 있는 문자열을 만나면 생성을 중단시킴 \n",
    ")\n",
    "genai.configure(api_key='AIzaSyCKv73cpWOu7mWuvgoMVm3xp__9b4TpzIE')\n",
    "model = genai.GenerativeModel('gemini-pro', generation_config=generation_config)\n",
    "\n",
    "user_queries = [\n",
    "    {\n",
    "        'role':'user', 'parts': [\"특정장소(카페)에서 점원이 특정 질문(손님 주문하시겠어요?)라는 질문에 대해 어떤 상황인지 1줄로 설명\"]\n",
    "    },\n",
    "    {\n",
    "        'role':'user',\n",
    "        'parts': [\"이 상황에서 손님이 주문하기 위한 멘트를 나열 해주세요. 이때 멘트는 5개 안으로 제한해주시고, 총 2가지 상황(주문, 용무)에 대한 멘트를 나열해주세요. 마지막으로 json 파일 형식으로 만들어주세요\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "history = []\n",
    "for user_query in user_queries:\n",
    "    history.append(user_query)\n",
    "    print(f'[사용자]: {user_query[\"parts\"][0]}')   \n",
    "    response = model.generate_content(history)\n",
    "    print(f'[모델]: {response.text}')   \n",
    "    history.append(response.candidates[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts {\n",
      "  text: \"카페에서 점원이 손님에게 주문을 묻는 상황\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(history[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1828758/4175914210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'안녕하세요'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DH/lib/python3.9/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "translator.translate('안녕하세요', dest='en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DH_2",
   "language": "python",
   "name": "dh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
